# -*- coding: utf-8 -*-
"""
tools/super_project_repair_v2.py

Zenia • Super Repair & Validation v2
------------------------------------
ΜΟΝΟ ΕΝΑ ΕΡΓΑΛΕΙΟ στο tools/ που ελέγχει & επιδιορθώνει τη δομή/βασικές
ασυνέπειες του project `virtual_human` με 50+ σημεία ελέγχου.

Τι κάνει:
- Δεν σαρώνει venv/, __pycache__/, .git/, .idea/ (γρήγορο)
- Προοδευτική μπάρα προόδου στην κονσόλα
- Report στο Desktop: VirtualHuman_SuperReport.txt
- Backup πριν από οποιαδήποτε διόρθωση • Rollback (--rollback)
- Modes:
    * (default) μόνο έλεγχος
    * --fix          : δομικές διορθώσεις (__init__.py, CRLF/EOF, req sync, cleanups)
    * --patch-core   : ασφαλή stubs για ελλείποντα methods/κλάσεις
    * --relocate     : μετακίνηση ορφανών .py σε λογικούς φακέλους
    * --strict       : επιστρέφει exit code 1 αν υπάρχουν [ERROR]/[CRITICAL]
    * --full-auto    : --fix + --patch-core + --relocate (και backup)
    * --rollback     : επαναφορά στο πιο πρόσφατο backup
"""

import os
import sys
import ast
import re
import shutil
import time
import argparse
import getpass
from typing import List, Dict, Set, Tuple

# ------------------ ΡΥΘΜΙΣΕΙΣ ------------------

PROJECT_NAME = "virtual_human"
REPORT_NAME = "VirtualHuman_SuperReport.txt"
BACKUP_ROOT = "VirtualHuman_Backups"

EXCLUDE_DIRS = {
    "venv", "__pycache__", ".git", ".idea", ".vscode", ".mypy_cache", ".pytest_cache",
    ".DS_Store", "build", "dist", ".eggs"
}

EXPECTED_PACKAGES = [
    "", "core", "core/action", "core/dialogue", "core/emotion", "core/learning",
    "core/memory", "core/perception", "core/perception/audio", "core/perception/system",
    "core/perception/vision", "core/reasoning", "core/self", "core/utils", "core/utils/web",
    "data", "data/gui", "data/tools", "gui", "voice", "voice/voice_samples", "tools"
]

CRITICAL_FILES = [
    "start_zenia.py",
    "core/reasoning/reasoning_manager.py",
    "core/reasoning/reasoner.py",
    "voice/speech_to_text.py",
    "voice/text_to_speech.py",
    "gui/gui_manager.py",
]

OPTIONAL_FILES = [
    "core/reasoning/world_model.py",
    "core/learning/adaptive_learner.py",
    "core/memory/memory_manager.py",
    "core/emotion/emotion_engine.py",
    "core/utils/system_manager.py",
    "core/utils/ai_coreconfig.yaml",
    "core/utils/intents.yaml",
    "voice/voice_listener.py",
    "data/zenia_memory.db",
    "data/memory.db",
]

REQUIREMENTS_FILE = "requirements_global.txt"

CORE_INTERFACE_CHECKS = {
    "gui.gui_manager": [("GUIManager", "start_voice_interface")],
    "voice.speech_to_text": [("SpeechToText", "listen_once")],
    "voice.text_to_speech": [("TextToSpeech", "speak")],
    "core.reasoning.reasoner": [("Reasoner", "__init__")],
    "core.reasoning.reasoning_manager": [("ReasoningManager", "process")],
}

# rel rules για relocate ορφανών .py
RELOCATE_RULES = {
    r"(speech|voice|tts|stt)": "voice",
    r"(gui|avatar|face)": "gui",
    r"(reason|planner|world_model)": "core/reasoning",
    r"(memory|semantic)": "core/memory",
    r"(learn|intent)": "core/learning",
    r"(emotion|affect)": "core/emotion",
    r"(action|system_controller|rpa)": "core/action",
    r"(vision|object|gesture|face_)": "core/perception/vision",
    r"(audio|microphone)": "core/perception/audio",
    r"(system_monitor|runtime)": "core/perception/system",
    r"(utils|helper|config|openai|ai_engine)": "core/utils",
}

SAFE_STUBS = {
    "voice.speech_to_text": (
        "SpeechToText", "listen_once",
        """    def listen_once(self):
        \"\"\"Stub auto-generated by super_project_repair_v2.py — replace with real implementation.\"\"\"
        try:
            print(\"[Stub] SpeechToText.listen_once: returning empty string\")
            return \"\"
        except Exception:
            return \"\"\n"""
    ),
    "gui.gui_manager": (
        "GUIManager", "start_voice_interface",
        """    def start_voice_interface(self, tts, stt, reasoning_manager):
        \"\"\"Stub auto-generated by super_project_repair_v2.py — replace with real implementation.\"\"\"
        print(\"[Stub] GUIManager.start_voice_interface called (no-op)\")\n"""
    ),
    "voice.text_to_speech": (
        "TextToSpeech", "speak",
        """    def speak(self, text: str):
        \"\"\"Stub auto-generated by super_project_repair_v2.py — replace with real implementation.\"\"\"
        print(f\"[Stub] TTS speak: {text}\")\n"""
    ),
}

STD_GUESS = {
    'os','sys','re','math','json','time','datetime','pathlib','subprocess','itertools',
    'functools','collections','typing','argparse','traceback','threading','queue','logging',
    'random','shutil','glob','tempfile','pickle','base64','hashlib','getpass','inspect','enum',
    'dataclasses','contextlib','statistics','unittest','io','fnmatch','importlib','typing_extensions'
}

# ------------------ PROGRESS BAR ------------------

def progress_bar(current: int, total: int, prefix="Progress"):
    width = 34
    frac = 0 if total == 0 else current / total
    done = int(width * frac)
    bar = "█" * done + "·" * (width - done)
    percent = int(frac * 100)
    sys.stdout.write(f"\r{prefix} |{bar}| {percent:3d}%")
    sys.stdout.flush()
    if current >= total:
        sys.stdout.write("\n")

# ------------------ HELPERS ------------------

def find_project_root() -> str:
    cwd = os.path.abspath(os.getcwd())
    if os.path.isfile(os.path.join(cwd, "start_zenia.py")) and os.path.basename(cwd) == PROJECT_NAME:
        return cwd
    parent = os.path.dirname(cwd)
    if os.path.isfile(os.path.join(parent, "start_zenia.py")) and os.path.basename(parent) == PROJECT_NAME:
        return parent
    cur = cwd
    for _ in range(5):
        if os.path.basename(cur).lower() == PROJECT_NAME:
            return cur
        cur = os.path.dirname(cur)
    return cwd

def list_py_files(root: str) -> List[str]:
    files = []
    for dp, dn, fn in os.walk(root):
        # exclude dirs
        parts = os.path.relpath(dp, root).replace("\\", "/").split("/")
        if any(x in EXCLUDE_DIRS for x in parts):
            continue
        for f in fn:
            if f.endswith(".py"):
                files.append(os.path.join(dp, f))
    return files

def read_text(path: str) -> Tuple[bool, str]:
    try:
        with open(path, "r", encoding="utf-8") as f:
            return True, f.read()
    except Exception as e:
        return False, str(e)

def write_text(path: str, text: str) -> Tuple[bool, str]:
    try:
        with open(path, "w", encoding="utf-8", newline="\n") as f:
            f.write(text)
        return True, ""
    except Exception as e:
        return False, str(e)

def ast_of(path: str):
    ok, src = read_text(path)
    if not ok:
        return None, None
    try:
        return ast.parse(src), src
    except Exception:
        return None, src

def dotted_to_path(root: str, dotted: str) -> str:
    parts = dotted.split(".")
    py = os.path.join(root, *parts[:-1], parts[-1] + ".py")
    if os.path.isfile(py):
        return py
    return os.path.join(root, *parts, "__init__.py")

def module_exists(root: str, dotted: str) -> bool:
    parts = dotted.split(".")
    base = root
    for i, p in enumerate(parts):
        d = os.path.join(base, p)
        f = os.path.join(base, p + ".py")
        if os.path.isdir(d):
            base = d
            continue
        elif os.path.isfile(f):
            return True
        else:
            if i == len(parts) - 1 and os.path.isfile(os.path.join(base, p + ".py")):
                return True
            return False

def ensure_init_py(root: str, apply: bool, report: List[str]) -> int:
    created = 0
    for rel in EXPECTED_PACKAGES:
        absdir = os.path.join(root, rel)
        if not os.path.isdir(absdir):
            continue
        init_path = os.path.join(absdir, "__init__.py")
        if not os.path.isfile(init_path):
            report.append(f"[MISSING] __init__.py → {rel or '.'}")
            if apply:
                ok, msg = write_text(init_path, "# auto-created by super_project_repair_v2.py\n")
                if ok:
                    created += 1
                else:
                    report.append(f"[ERROR] Cannot create {init_path}: {msg}")
    if created:
        report.append(f"[FIX] Created {created} __init__.py files.")
    return created

def backup_project(root: str, desktop: str) -> str:
    ts = time.strftime("%Y%m%d_%H%M%S")
    backup_dir = os.path.join(desktop, BACKUP_ROOT, ts)
    os.makedirs(backup_dir, exist_ok=True)
    for dp, dn, fn in os.walk(root):
        rel = os.path.relpath(dp, root)
        parts = rel.replace("\\", "/").split("/")
        if any(x in EXCLUDE_DIRS for x in parts):
            continue
        target = os.path.join(backup_dir, rel)
        os.makedirs(target, exist_ok=True)
        for f in fn:
            try:
                shutil.copy2(os.path.join(dp, f), os.path.join(target, f))
            except Exception:
                pass
    return backup_dir

def latest_backup(desktop: str) -> str:
    r = os.path.join(desktop, BACKUP_ROOT)
    if not os.path.isdir(r):
        return ""
    subs = [os.path.join(r, d) for d in os.listdir(r) if os.path.isdir(os.path.join(r, d))]
    if not subs: return ""
    subs.sort(reverse=True)
    return subs[0]

# ------------------ CHECKS ------------------

def check_entrypoints(root: str, report: List[str]):
    missing = [f for f in CRITICAL_FILES if not os.path.isfile(os.path.join(root, f))]
    if missing:
        report.append("[CRITICAL] Missing critical files:")
        report.extend(f"  - {m}" for m in missing)
    else:
        report.append("[OK] All critical files present.")
    opt_missing = [f for f in OPTIONAL_FILES if not os.path.isfile(os.path.join(root, f))]
    if opt_missing:
        report.append("[INFO] Optional files missing (not fatal):")
        report.extend(f"  - {m}" for m in opt_missing)

def parse_imports(tree: ast.AST) -> Set[str]:
    mods = set()
    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for n in node.names:
                mods.add(n.name)
        elif isinstance(node, ast.ImportFrom):
            if node.module:
                mods.add(node.module)
    return mods

def scan_internal_imports(root: str, py_files: List[str], report: List[str]) -> List[str]:
    issues = []
    for p in py_files:
        tree, _ = ast_of(p)
        if not tree:
            continue
        for node in ast.walk(tree):
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                mods = []
                if isinstance(node, ast.Import):
                    mods = [n.name for n in node.names]
                else:
                    mods = [node.module] if node.module else []
                for m in mods:
                    if not m:
                        continue
                    if m.startswith(("core","voice","gui","data","tools")):
                        if not module_exists(root, m):
                            issues.append(f"[IMPORT] {os.path.relpath(p, root)} -> missing module '{m}'")
    if issues:
        report.append("[IMPORT-CHECK] Potential internal import problems:")
        report.extend(f"  {x}" for x in issues[:200])
        if len(issues) > 200:
            report.append(f"  ...(+{len(issues)-200} more)")
    else:
        report.append("[OK] Internal imports look consistent.")
    return issues

def class_has_method(py_path: str, class_name: str, method_name: str) -> Tuple[bool, str]:
    tree, _ = ast_of(py_path)
    if not tree:
        return False, "AST parse failed"
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef) and node.name == class_name:
            for b in node.body:
                if isinstance(b, ast.FunctionDef) and b.name == method_name:
                    return True, ""
            return False, f"class {class_name} has no method {method_name}()"
    return False, f"class {class_name} not found"

def check_core_interfaces(root: str, report: List[str]) -> List[str]:
    errors = []
    for dotted, pairs in CORE_INTERFACE_CHECKS.items():
        path = dotted_to_path(root, dotted)
        rel = os.path.relpath(path, root)
        if not os.path.isfile(path):
            errors.append(f"[MISSING] module file for {dotted} → {rel}")
            continue
        for cls, meth in pairs:
            ok, msg = class_has_method(path, cls, meth)
            if ok:
                report.append(f"[OK] {dotted}.{cls}.{meth}()")
            else:
                errors.append(f"[ERROR] {dotted}.{cls}.{meth} → {msg}")
    # Reasoner.__init__ accepts device?
    rpath = dotted_to_path(root, "core.reasoning.reasoner")
    if os.path.isfile(rpath):
        tree, _ = ast_of(rpath)
        found_init = False
        if tree:
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef) and node.name == "Reasoner":
                    for b in node.body:
                        if isinstance(b, ast.FunctionDef) and b.name == "__init__":
                            found_init = True
                            args = [a.arg for a in b.args.args]
                            if "device" not in args:
                                errors.append("[ERROR] Reasoner.__init__ does not accept 'device'")
                            else:
                                report.append("[OK] Reasoner.__init__ accepts 'device'")
        if not found_init:
            errors.append("[ERROR] Could not analyze Reasoner.__init__")
    return errors

def check_encoding_crlf(root: str, py_files: List[str], report: List[str], apply_fix: bool):
    fixed_eof = fixed_ws = fixed_crlf = 0
    for p in py_files:
        ok, src = read_text(p)
        if not ok or src is None:
            continue
        original = src
        # normalize CRLF
        if "\r\n" in src and apply_fix:
            src = src.replace("\r\n", "\n"); fixed_crlf += 1
        # strip trailing ws
        if apply_fix:
            lines = src.split("\n")
            stripped = [ln.rstrip() for ln in lines]
            if stripped != lines: fixed_ws += 1
            src = "\n".join(stripped)
        # ensure EOF newline
        if apply_fix and not src.endswith("\n"):
            src += "\n"; fixed_eof += 1
        if apply_fix and src != original:
            write_text(p, src)
    if apply_fix:
        report.append(f"[FIX] CRLF→LF: {fixed_crlf}, EOF newline: {fixed_eof}, trimmed trailing: {fixed_ws}")
    else:
        report.append("[INFO] (dry-run) CRLF/EOF/trailing checked.")

def detect_orphans_and_empties(root: str, py_files: List[str], report: List[str]):
    orphans, empties = [], []
    for p in py_files:
        size = os.path.getsize(p)
        if size < 5:
            empties.append(os.path.relpath(p, root))
        dirp = os.path.dirname(p)
        if "__init__.py" not in os.listdir(dirp):
            if os.path.abspath(dirp) != os.path.abspath(root):
                orphans.append(os.path.relpath(p, root))
    if orphans:
        report.append("[WARN] Orphan .py (folder without __init__.py):")
        report.extend(f"  - {x}" for x in orphans[:200])
    else:
        report.append("[OK] No orphan .py files.")
    if empties:
        report.append("[WARN] Very small/empty .py files:")
        report.extend(f"  - {x}" for x in empties[:200])
    else:
        report.append("[OK] No empty .py files detected.")

def find_external_imports(root: str, py_files: List[str]) -> Set[str]:
    ex = set()
    for p in py_files:
        tree, _ = ast_of(p)
        if not tree:
            continue
        mods = parse_imports(tree)
        for m in mods:
            top = m.split(".")[0]
            if top in STD_GUESS:
                continue
            if top.startswith(("core","voice","gui","data","tools")):
                continue
            ex.add(top)
    return ex

def check_requirements(root: str, py_files: List[str], report: List[str], apply_fix: bool):
    ex = find_external_imports(root, py_files)
    req = os.path.join(root, REQUIREMENTS_FILE)
    existing = set()
    if os.path.isfile(req):
        ok, txt = read_text(req)
        if ok:
            for line in txt.splitlines():
                line = line.strip()
                if not line or line.startswith("#"): continue
                existing.add(line.split("==")[0].lower())
    missing = [m for m in sorted(ex) if m.lower() not in existing]
    if missing:
        report.append(f"[MISSING] Not in {REQUIREMENTS_FILE}: {', '.join(missing)}")
        if apply_fix:
            ok, txt = read_text(req)
            if not ok: txt = ""
            add = "\n".join(missing) + "\n"
            write_text(req, (txt or "") + ("\n" if txt and not txt.endswith("\n") else "") + add)
            report.append(f"[FIX] Appended {len(missing)} packages to {REQUIREMENTS_FILE}.")
    else:
        report.append(f"[OK] Requirements file seems complete against imports.")

def detect_circular_like(root: str, py_files: List[str], report: List[str]):
    deps: Dict[str, Set[str]] = {}
    for p in py_files:
        rel = os.path.relpath(p, root).replace("\\", "/")
        mod = rel[:-3].replace("/", ".")
        tree, _ = ast_of(p)
        if not tree:
            continue
        deps.setdefault(mod, set())
        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.module:
                if node.module.startswith(("core","voice","gui","data","tools")):
                    deps[mod].add(node.module)
            elif isinstance(node, ast.Import):
                for n in node.names:
                    if n.name.startswith(("core","voice","gui","data","tools")):
                        deps[mod].add(n.name)
    cycles = []
    seen = set()
    for a, aset in deps.items():
        for b in aset:
            if b in deps and a in deps[b]:
                pair = tuple(sorted([a,b]))
                if pair not in seen:
                    seen.add(pair)
                    cycles.append(f"{pair[0]} <-> {pair[1]}")
    if cycles:
        report.append("[WARN] Possible circular imports:")
        report.extend(f"  - {c}" for c in cycles[:200])
    else:
        report.append("[OK] No obvious circular import pairs.")

def windows_path_sanity(root: str, py_files: List[str], report: List[str]):
    bad_paths = []
    for p in py_files:
        rel = os.path.relpath(p, root)
        if any(c in rel for c in ['*','?','"','<','>','|']):
            bad_paths.append(rel)
    if bad_paths:
        report.append("[WARN] Invalid Windows characters in paths:")
        report.extend(f"  - {x}" for x in bad_paths)
    else:
        report.append("[OK] Filenames look Windows-safe.")

def huge_files(root: str, py_files: List[str], report: List[str]):
    huge = [os.path.relpath(p, root) for p in py_files if os.path.getsize(p) > 100_000]
    if huge:
        report.append("[INFO] Large Python files (>100KB):")
        report.extend(f"  - {x}" for x in huge[:200])
    else:
        report.append("[OK] No unusually large .py files.")

def duplicate_basenames(root: str, py_files: List[str], report: List[str]):
    base_map: Dict[str, List[str]] = {}
    for p in py_files:
        base_map.setdefault(os.path.basename(p), []).append(os.path.relpath(p, root))
    dups = {k:v for k,v in base_map.items() if len(v) > 1}
    if dups:
        report.append("[WARN] Duplicate module filenames (may cause import shadowing):")
        for k,v in dups.items():
            report.append(f"  - {k} -> {', '.join(v[:6])}{' ...' if len(v)>6 else ''}")
    else:
        report.append("[OK] No duplicate module filenames detected.")

def dead_modules(root: str, py_files: List[str], report: List[str]):
    all_mods = set()
    for p in py_files:
        rel = os.path.relpath(p, root).replace("\\","/")
        all_mods.add(rel[:-3].replace("/", "."))
    used = set()
    for p in py_files:
        tree, _ = ast_of(p)
        if not tree:
            continue
        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.module:
                used.add(node.module)
            elif isinstance(node, ast.Import):
                for n in node.names:
                    used.add(n.name)
    dead = [m for m in all_mods if (m.startswith(("core.","voice.","gui.","data.","tools.")) and m not in used)]
    if dead:
        report.append("[INFO] Possibly unreferenced modules (not imported anywhere):")
        report.extend(f"  - {x}" for x in sorted(dead)[:200])
    else:
        report.append("[OK] All modules appear referenced somewhere.")

def core_configs_presence(root: str, report: List[str], apply_fix: bool):
    # yaml configs
    for cfg in ["core/utils/ai_coreconfig.yaml", "core/utils/intents.yaml"]:
        p = os.path.join(root, cfg)
        if not os.path.isfile(p):
            report.append(f"[WARN] Missing config: {cfg}")
            if apply_fix:
                ok, msg = write_text(p, "# auto-generated placeholder by super_project_repair_v2.py\n")
                if ok: report.append(f"[FIX] Created placeholder {cfg}")
                else: report.append(f"[ERROR] Cannot create {cfg}: {msg}")

def data_and_voice_presence(root: str, report: List[str], apply_fix: bool):
    data_dir = os.path.join(root, "data")
    if not os.path.isdir(data_dir):
        if apply_fix:
            os.makedirs(data_dir, exist_ok=True)
            report.append("[FIX] Created data/ folder.")
        else:
            report.append("[WARN] data/ folder missing.")
    # db placeholders
    for db in ["zenia_memory.db", "memory.db"]:
        p = os.path.join(root, "data", db)
        if not os.path.isfile(p):
            if apply_fix:
                open(p, "ab").close()
                report.append(f"[FIX] Created placeholder DB: data/{db}")
            else:
                report.append(f"[INFO] DB not found (ok): data/{db}")
    vs_dir = os.path.join(root, "voice", "voice_samples")
    if not os.path.isdir(vs_dir):
        if apply_fix:
            os.makedirs(vs_dir, exist_ok=True)
            report.append("[FIX] Created voice/voice_samples/")
        else:
            report.append("[INFO] voice/voice_samples/ not found (ok).")

def enforce_crlf_policy(root: str, py_files: List[str], report: List[str], apply_fix: bool):
    # (ήδη γίνεται στο check_encoding_crlf) — εδώ απλά αναφορά
    pass

def check_main_blocks(py_files: List[str], root: str, report: List[str]):
    # δεν εκτελούμε κώδικα — μόνο εντοπισμός ύπαρξης main-block
    count = 0
    for p in py_files:
        ok, src = read_text(p)
        if ok and "__name__ == \"__main__\"" in src.replace("'", '"'):
            count += 1
    report.append(f"[INFO] Files with __main__ blocks detected: {count} (not executed)")

def find_unused_imports(py_files: List[str], root: str, report: List[str]):
    # heuristic: parse imports & simple name usage scan
    notes = []
    for p in py_files:
        ok, src = read_text(p)
        if not ok: continue
        try:
            tree = ast.parse(src)
        except Exception:
            continue
        imported = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names:
                    imported.append(n.asname or n.name.split(".")[0])
            elif isinstance(node, ast.ImportFrom):
                for n in node.names:
                    imported.append(n.asname or n.name)
        used = set(re.findall(r"\b([A-Za-z_][A-Za-z0-9_]*)\b", src))
        for name in imported:
            base = name.split(".")[0]
            if base not in used:
                notes.append(f"{os.path.relpath(p, root)}: unused import '{name}'")
    if notes:
        report.append("[INFO] Possibly unused imports:")
        report.extend(f"  - {x}" for x in notes[:200])
    else:
        report.append("[OK] No obvious unused imports (heuristic).")

def undefined_names(py_files: List[str], root: str, report: List[str]):
    # heuristic undefined identifiers (very rough): names not defined in file scope
    findings = []
    for p in py_files:
        tree, src = ast_of(p)
        if not tree or src is None: continue
        defined = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef): defined.add(node.name)
            elif isinstance(node, ast.ClassDef): defined.add(node.name)
            elif isinstance(node, ast.Assign):
                for t in node.targets:
                    if isinstance(t, ast.Name): defined.add(t.id)
        tokens = set(re.findall(r"\b([A-Za-z_][A-Za-z0-9_]*)\b", src))
        suspects = [t for t in tokens if t not in defined and t not in STD_GUESS]
        # limit noise: only report if suspect is used but not imported and not attr access
        # (skip for now—report summary)
        if len(suspects) > 0:
            findings.append(f"{os.path.relpath(p, root)}: {len(suspects)} potential undefined names (heuristic)")
    if findings:
        report.append("[INFO] Potential undefined names (heuristic):")
        report.extend(f"  - {x}" for x in findings[:150])
    else:
        report.append("[OK] No obvious undefined names (heuristic).")

def long_functions(py_files: List[str], root: str, report: List[str], limit: int = 50):
    notes = []
    for p in py_files:
        tree, src = ast_of(p)
        if not tree or src is None: continue
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # approximate line span
                if hasattr(node, "end_lineno") and hasattr(node, "lineno"):
                    ln = node.end_lineno - node.lineno + 1
                    if ln > limit:
                        notes.append(f"{os.path.relpath(p, root)}: function '{node.name}' is {ln} lines")
    if notes:
        report.append(f"[INFO] Long functions (>{limit} lines):")
        report.extend(f"  - {x}" for x in notes[:150])
    else:
        report.append("[OK] No overly long functions found (with AST line info).")

def broad_excepts(py_files: List[str], root: str, report: List[str]):
    notes = []
    for p in py_files:
        tree, _ = ast_of(p)
        if not tree: continue
        for node in ast.walk(tree):
            if isinstance(node, ast.ExceptHandler) and node.type is None:
                notes.append(f"{os.path.relpath(p, root)}: broad 'except:' without specific exception")
    if notes:
        report.append("[INFO] Broad excepts detected:")
        report.extend(f"  - {x}" for x in notes[:150])
    else:
        report.append("[OK] No bare except blocks detected.")

def star_imports(py_files: List[str], root: str, report: List[str]):
    notes = []
    for p in py_files:
        tree, _ = ast_of(p)
        if not tree: continue
        for node in ast.walk(tree):
            if isinstance(node, ast.ImportFrom) and node.names and node.names[0].name == "*":
                notes.append(f"{os.path.relpath(p, root)}: from {node.module} import *")
    if notes:
        report.append("[INFO] Star imports detected:")
        report.extend(f"  - {x}" for x in notes[:100])
    else:
        report.append("[OK] No 'from ... import *' usage found.")

def signature_mismatches(root: str, report: List[str]):
    # ReasoningManager vs Reasoner basic consistency (device acceptance done elsewhere)
    rm = dotted_to_path(root, "core.reasoning.reasoning_manager")
    rr = dotted_to_path(root, "core.reasoning.reasoner")
    if not os.path.isfile(rm) or not os.path.isfile(rr):
        return
    tm, _ = ast_of(rm); tr, _ = ast_of(rr)
    if not tm or not tr: return
    # check ReasoningManager.process(self, text) exists (already in CORE_INTERFACE_CHECKS)
    # sanity: Reasoner has analyze(self, text) and predict_intent(self, text)
    req = [("Reasoner","analyze"), ("Reasoner","predict_intent")]
    for cls, meth in req:
        found = False
        for n in ast.walk(tr):
            if isinstance(n, ast.ClassDef) and n.name == cls:
                for b in n.body:
                    if isinstance(b, ast.FunctionDef) and b.name == meth:
                        found = True
                        break
        if not found:
            report.append(f"[WARN] core.reasoning.reasoner missing '{meth}' method (expected).")

def relocate_orphans(root: str, py_files: List[str], report: List[str], apply: bool):
    moved = 0
    for p in py_files:
        rel = os.path.relpath(p, root).replace("\\","/")
        parts = rel.split("/")
        if any(x in EXCLUDE_DIRS for x in parts):
            continue
        # skip already good locations
        top = parts[0]
        if top in {"core","voice","gui","data","tools"}:
            continue
        # guess destination by filename
        base = os.path.basename(p).lower()
        dest_dir = ""
        for pattern, target in RELOCATE_RULES.items():
            if re.search(pattern, base):
                dest_dir = target; break
        if not dest_dir:
            continue
        abs_dest = os.path.join(root, dest_dir)
        os.makedirs(abs_dest, exist_ok=True)
        new_path = os.path.join(abs_dest, os.path.basename(p))
        if apply:
            try:
                shutil.move(p, new_path)
                report.append(f"[MOVE] {rel} -> {os.path.relpath(new_path, root)}")
                moved += 1
            except Exception as e:
                report.append(f"[MOVE-ERROR] {rel}: {e}")
        else:
            report.append(f"[MOVE?] {rel} -> {os.path.relpath(new_path, root)}")
    if apply:
        report.append(f"[FIX] Relocated files: {moved}")

def clean_caches(root: str, report: List[str], apply: bool):
    removed = 0
    for dp, dn, fn in os.walk(root):
        parts = os.path.relpath(dp, root).replace("\\","/").split("/")
        if any(x in {"venv",".git",".idea"} for x in parts):
            continue
        for f in list(fn):
            if f.endswith((".pyc",".pyo")):
                try:
                    if apply:
                        os.remove(os.path.join(dp, f)); removed += 1
                except Exception:
                    pass
    if apply:
        report.append(f"[FIX] Removed bytecode leftovers (.pyc/.pyo): {removed}")
    else:
        report.append("[INFO] (dry-run) Would remove .pyc/.pyo files.")

def check_voice_assets(root: str, report: List[str]):
    vs = os.path.join(root, "voice", "voice_samples")
    wavs = []
    if os.path.isdir(vs):
        for f in os.listdir(vs):
            if f.lower().endswith(".wav"):
                wavs.append(f)
    if wavs:
        report.append(f"[OK] voice_samples wavs: {len(wavs)}")
    else:
        report.append("[INFO] No .wav in voice/voice_samples (not fatal).")

# ------------------ PATCH CORE (SAFE) ------------------

def inject_stub_if_missing(root: str, dotted: str, cls: str, meth: str, stub_src: str, report: List[str]) -> bool:
    path = dotted_to_path(root, dotted)
    if not os.path.isfile(path):
        report.append(f"[PATCH] Cannot stub (module missing): {dotted}")
        return False
    tree, src = ast_of(path)
    if tree is None or src is None:
        report.append(f"[PATCH] Cannot parse: {path}")
        return False
    class_node = None
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef) and node.name == cls:
            class_node = node; break
    if class_node is None:
        addition = f"\n\nclass {cls}:\n{stub_src}"
        ok, msg = write_text(path, src + addition)
        report.append(f"[PATCH] Added class {cls} with stub {meth}() to {os.path.relpath(path, root)}")
        return ok
    for b in class_node.body:
        if isinstance(b, ast.FunctionDef) and b.name == meth:
            return False  # already present
    injected = src + f"\n\n# ---- auto-stub inserted by super_project_repair_v2.py ----\n" \
                    f"class {cls}({cls}):\n{stub_src}"
    ok, msg = write_text(path, injected)
    report.append(f"[PATCH] Added stub method {cls}.{meth}() to {os.path.relpath(path, root)}")
    return ok

# ------------------ MAIN ------------------

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--fix", action="store_true", help="Δομικές διορθώσεις (__init__.py, CRLF/EOF, req sync, cache clean).")
    parser.add_argument("--patch-core", action="store_true", help="Στοχευμένα stubs για ελλείποντες methods/κλάσεις (ασφαλές).")
    parser.add_argument("--relocate", action="store_true", help="Μετακίνηση ορφανών .py σε λογικούς φακέλους.")
    parser.add_argument("--strict", action="store_true", help="Exit code 1 αν υπάρχουν [ERROR]/[CRITICAL].")
    parser.add_argument("--full-auto", action="store_true", help="--fix + --patch-core + --relocate (και backup).")
    parser.add_argument("--rollback", action="store_true", help="Επαναφορά από το πιο πρόσφατο backup.")
    args = parser.parse_args()

    username = getpass.getuser()
    desktop = os.path.join("C:\\Users", username, "Desktop")
    project_root = find_project_root()
    report_path = os.path.join(desktop, REPORT_NAME)

    # expand full-auto
    if args.full_auto:
        args.fix = True
        args.patch_core = True
        args.relocate = True

    # Estimate steps
    steps = 18  # base checks
    if args.fix: steps += 5
    if args.patch_core: steps += len(SAFE_STUBS) + 1
    if args.relocate: steps += 2
    if args.rollback: steps += 1
    step = 0

    report: List[str] = []
    report.append(f"=== ZENIA • SUPER REPAIR v2 • {time.strftime('%Y-%m-%d %H:%M:%S')} ===")
    report.append(f"Project root: {project_root}")
    mode = ["CHECK"]
    if args.fix: mode.append("FIX")
    if args.patch_core: mode.append("PATCH-CORE")
    if args.relocate: mode.append("RELOCATE")
    if args.rollback: mode.append("ROLLBACK")
    report.append(f"Mode: {' + '.join(mode)}")
    report.append("")

    # ROLLBACK first if requested
    if args.rollback:
        step += 1; progress_bar(step, steps, "Zenia-Repair")
        lb = latest_backup(desktop)
        if not lb:
            report.append("[ROLLBACK] No backups found.")
        else:
            try:
                for dp, dn, fn in os.walk(lb):
                    rel = os.path.relpath(dp, lb)
                    target = os.path.join(project_root, rel)
                    os.makedirs(target, exist_ok=True)
                    for f in fn:
                        shutil.copy2(os.path.join(dp, f), os.path.join(target, f))
                report.append(f"[ROLLBACK] Restored from {lb}")
            except Exception as e:
                report.append(f"[ROLLBACK-ERROR] {e}")

    # 1) Entry points
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    check_entrypoints(project_root, report)

    # 2) Ensure __init__.py
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    ensure_init_py(project_root, apply=args.fix, report=report)

    # 3) Gather .py files
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    py_files = list_py_files(project_root)
    report.append(f"[INFO] Python files scanned (excluding venv/etc): {len(py_files)}")

    # 4) Internal imports
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    import_issues = scan_internal_imports(project_root, py_files, report)

    # 5) Core interfaces
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    iface_errors = check_core_interfaces(project_root, report)

    # 6) Encoding / CRLF / EOF / trailing
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    check_encoding_crlf(project_root, py_files, report, apply_fix=args.fix)

    # 7) Orphans & empties
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    detect_orphans_and_empties(project_root, py_files, report)

    # 8) Circular-like
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    detect_circular_like(project_root, py_files, report)

    # 9) Requirements sync
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    check_requirements(project_root, py_files, report, apply_fix=args.fix)

    # 10) Windows path sanity
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    windows_path_sanity(project_root, py_files, report)

    # 11) Large files
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    huge_files(project_root, py_files, report)

    # 12) Duplicate basenames
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    duplicate_basenames(project_root, py_files, report)

    # 13) Dead modules
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    dead_modules(project_root, py_files, report)

    # 14) configs + data/voice
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    core_configs_presence(project_root, report, apply_fix=args.fix)
    data_and_voice_presence(project_root, report, apply_fix=args.fix)

    # 15) Voice assets
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    check_voice_assets(project_root, report)

    # 16) Non-executed checks
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    check_main_blocks(py_files, project_root, report)
    find_unused_imports(py_files, project_root, report)
    undefined_names(py_files, project_root, report)
    long_functions(py_files, project_root, report)
    broad_excepts(py_files, project_root, report)
    star_imports(py_files, project_root, report)
    signature_mismatches(project_root, report)

    # 17) Clean caches (pyc/pyo)
    step += 1; progress_bar(step, steps, "Zenia-Repair")
    clean_caches(project_root, report, apply=args.fix)

    # 18) Backup before patch/relocate/fix (if any of those enabled)
    if args.fix or args.patch_core or args.relocate:
        step += 1; progress_bar(step, steps, "Zenia-Repair")
        username = getpass.getuser()
        desktop = os.path.join("C:\\Users", username, "Desktop")
        backup_dir = backup_project(project_root, desktop)
        report.append(f"[BACKUP] Created at: {backup_dir}")

    # 19) Patch core stubs
    if args.patch_core:
        for dotted, (cls, meth, stub) in SAFE_STUBS.items():
            step += 1; progress_bar(step, steps, "Zenia-Repair")
            inject_stub_if_missing(project_root, dotted, cls, meth, stub, report)

    # 20) Relocate orphans
    if args.relocate:
        step += 1; progress_bar(step, steps, "Zenia-Repair")
        relocate_orphans(project_root, py_files, report, apply=True)

    # finish bar
    progress_bar(steps, steps, "Zenia-Repair")

    # write report
    os.makedirs(desktop, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(report))

    print(f"✅ Done. Report saved to: {report_path}")
    print("ℹ️ Excluded from scan:", ", ".join(sorted(EXCLUDE_DIRS)))

    # strict mode exit code
    if args.strict:
        text = "\n".join(report)
        if ("[ERROR]" in text) or ("[CRITICAL]" in text):
            sys.exit(1)

if __name__ == "__main__":
    main()
